{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c857b-08cf-4848-8d01-64332b9f3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training과 Validation 경로 설정\n",
    "train_part_folder = './extracted_files/023.국회_회의록_기반_지식검색_데이터/3.개방데이터/1.데이터/Training/02.라벨링데이터'\n",
    "val_part_folder = './extracted_files/023.국회_회의록_기반_지식검색_데이터/3.개방데이터/1.데이터/Validation/02.라벨링데이터'\n",
    "\n",
    "# 병합된 ZIP 파일이 저장될 경로 설정\n",
    "train_zip_output_folder = './extracted_files/Training'\n",
    "val_zip_output_folder = './extracted_files/Validation'\n",
    "\n",
    "# 병합된 ZIP 파일 저장 경로 설정\n",
    "train_output_zip_files = {\n",
    "    \"TL_국정감사.zip\": os.path.join(train_zip_output_folder, \"TL_국정감사.zip\"),\n",
    "    \"TL_본회의.zip\": os.path.join(train_zip_output_folder, \"TL_본회의.zip\"),\n",
    "    \"TL_소위원회.zip\": os.path.join(train_zip_output_folder, \"TL_소위원회.zip\"),\n",
    "    \"TL_예산결산특별위원회.zip\": os.path.join(train_zip_output_folder, \"TL_예산결산특별위원회.zip\"),\n",
    "    \"TL_특별위원회.zip\": os.path.join(train_zip_output_folder, \"TL_특별위원회.zip\"),\n",
    "}\n",
    "\n",
    "val_output_zip_files = {\n",
    "    \"VL_국정감사.zip\": os.path.join(val_zip_output_folder, \"VL_국정감사.zip\"),\n",
    "    \"VL_본회의.zip\": os.path.join(val_zip_output_folder, \"VL_본회의.zip\"),\n",
    "    \"VL_소위원회.zip\": os.path.join(val_zip_output_folder, \"VL_소위원회.zip\"),\n",
    "    \"VL_예산결산특별위원회.zip\": os.path.join(val_zip_output_folder, \"VL_예산결산특별위원회.zip\"),\n",
    "    \"VL_특별위원회.zip\": os.path.join(val_zip_output_folder, \"VL_특별위원회.zip\"),\n",
    "}\n",
    "\n",
    "# 1. 분할 파일을 병합하는 함수\n",
    "def merge_part_files(part_folder, output_file_prefix):\n",
    "    part_files = sorted([f for f in os.listdir(part_folder) if f.endswith('.part0')], key=lambda x: x)\n",
    "    for part_file in part_files:\n",
    "        zip_file_key = part_file.replace('.zip.part0', '.zip')  # 중복 확장자 방지\n",
    "        if zip_file_key in output_file_prefix:\n",
    "            zip_file = output_file_prefix[zip_file_key]\n",
    "\n",
    "            # 폴더가 존재하지 않으면 생성\n",
    "            os.makedirs(os.path.dirname(zip_file), exist_ok=True)\n",
    "            \n",
    "            part_file_path = os.path.join(part_folder, part_file)\n",
    "            with open(zip_file, 'wb') as merged_file:\n",
    "                with open(part_file_path, 'rb') as pf:\n",
    "                    merged_file.write(pf.read())\n",
    "                print(f\"{part_file} 병합 완료. ZIP 파일 저장 경로: {zip_file}\")\n",
    "        else:\n",
    "            print(f\"Warning: {zip_file_key}에 해당하는 출력 경로가 없습니다.\")\n",
    "\n",
    "# 2. ZIP 파일을 해제하는 함수\n",
    "def extract_zip_files(zip_file, extract_to):\n",
    "    if not os.path.exists(zip_file):\n",
    "        print(f\"ZIP 파일이 존재하지 않습니다: {zip_file}\")\n",
    "        return\n",
    "    \n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "        print(f\"{zip_file} 압축 해제 완료. 압축 해제 경로: {extract_to}\")\n",
    "\n",
    "# 3. Training 라벨링 데이터 병합 및 압축 해제\n",
    "merge_part_files(train_part_folder, train_output_zip_files)\n",
    "\n",
    "# Training ZIP 파일 해제\n",
    "for zip_file, output_path in train_output_zip_files.items():\n",
    "    extract_zip_files(output_path, os.path.join(train_zip_output_folder, os.path.splitext(zip_file)[0]))\n",
    "\n",
    "# 4. Validation 라벨링 데이터 병합 및 압축 해제\n",
    "merge_part_files(val_part_folder, val_output_zip_files)\n",
    "\n",
    "# Validation ZIP 파일 해제\n",
    "for zip_file, output_path in val_output_zip_files.items():\n",
    "    extract_zip_files(output_path, os.path.join(val_zip_output_folder, os.path.splitext(zip_file)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87af2959-cde1-4c66-8edc-18f3dd54a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gradio as gr\n",
    "import zipfile\n",
    "import pickle\n",
    "import MeCab\n",
    "\n",
    "# 1. Mecab 형태소 분석기 초기화\n",
    "mecab = MeCab.Tagger()\n",
    "\n",
    "# 2. ZIP 파일 압축 해제 함수\n",
    "def extract_zip_files(zip_folder, extract_to):\n",
    "    zip_files = [f for f in os.listdir(zip_folder) if f.endswith('.zip')]\n",
    "    for zip_file in zip_files:\n",
    "        zip_file_path = os.path.join(zip_folder, zip_file)\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "            print(f\"{zip_file_path} 압축 해제 완료. 압축 해제 경로: {extract_to}\")\n",
    "\n",
    "# 3. Mecab을 이용한 형태소 분석 및 전처리\n",
    "def preprocess_with_mecab(text):\n",
    "    \"\"\"Mecab을 사용하여 텍스트를 전처리하는 함수\"\"\"\n",
    "    words = mecab.parse(text).split()  # 형태소 단위로 나누기\n",
    "    return \" \".join(words)\n",
    "\n",
    "# 4. 데이터 로드 함수\n",
    "def load_single_file(file_path):\n",
    "    \"\"\"한 개의 JSON 파일을 로드하고 형태소 분석을 적용하는 함수\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            # 필요한 데이터를 처리\n",
    "            question = data.get('question', {}).get('comment')  # 질문 필드\n",
    "            context = data.get('context')  # 문맥 필드\n",
    "            answer = data.get('answer', {}).get('comment')  # 답변 필드\n",
    "\n",
    "            # Mecab으로 전처리\n",
    "            if question:\n",
    "                question = preprocess_with_mecab(question)\n",
    "            if context:\n",
    "                context = preprocess_with_mecab(context)\n",
    "            if answer:\n",
    "                answer = preprocess_with_mecab(answer)\n",
    "\n",
    "            # answer_start는 문맥 내에서 답변 시작 위치를 찾음\n",
    "            answer_start = context.find(answer) if answer in context else -1\n",
    "\n",
    "            if question and context and answer and answer_start != -1:\n",
    "                return (question, context, answer, answer_start)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON 디코딩 오류: {e}, 파일: {file_path}\")\n",
    "    return None\n",
    "\n",
    "# 5. 캐시 관련 함수\n",
    "CACHE_FILE = 'data_cache.pkl'\n",
    "\n",
    "def load_data_from_cache(cache_file):\n",
    "    \"\"\"캐시된 데이터를 불러오는 함수\"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"캐시된 데이터를 불러오는 중: {cache_file}\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def save_data_to_cache(data, cache_file):\n",
    "    \"\"\"데이터를 캐시에 저장하는 함수\"\"\"\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"데이터를 캐시에 저장했습니다: {cache_file}\")\n",
    "\n",
    "# 6. 병렬로 데이터 로드\n",
    "def load_data_parallel(data_folder):\n",
    "    \"\"\"여러 개의 파일을 병렬로 로드하는 함수\"\"\"\n",
    "    qa_pairs = []\n",
    "    print(f\"데이터 폴더 경로: {data_folder}\")\n",
    "\n",
    "    # 파일 리스트 가져오기\n",
    "    files = [os.path.join(root, file)\n",
    "             for root, _, files in os.walk(data_folder)\n",
    "             for file in files if file.endswith('.json')]\n",
    "\n",
    "    print(f\"발견된 파일들: {len(files)}개\")\n",
    "\n",
    "    # 각 파일을 병렬로 로드\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(load_single_file, files))\n",
    "\n",
    "    # None이 아닌 결과만 모으기\n",
    "    qa_pairs = [result for result in results if result is not None]\n",
    "\n",
    "    print(f\"로드된 QA 쌍 개수: {len(qa_pairs)}\")\n",
    "    return qa_pairs\n",
    "\n",
    "# 7. 캐시와 함께 데이터 로드 함수\n",
    "def load_data_with_cache(data_folder, cache_file):\n",
    "    \"\"\"캐시를 사용해 데이터를 로드하는 함수\"\"\"\n",
    "    # 캐시에서 데이터를 불러오려고 시도\n",
    "    data = load_data_from_cache(cache_file)\n",
    "    if data is not None:\n",
    "        return data\n",
    "\n",
    "    # 캐시된 데이터가 없으면 병렬로 데이터 로드\n",
    "    data = load_data_parallel(data_folder)\n",
    "    save_data_to_cache(data, cache_file)\n",
    "    return data\n",
    "\n",
    "# 8. 데이터 경로 설정 및 압축 해제\n",
    "train_zip_folder = './extracted_files/Training'\n",
    "val_zip_folder = './extracted_files/Validation'\n",
    "\n",
    "# 압축 해제 경로\n",
    "train_extracted_path = './extracted_files/Training/unzipped'\n",
    "val_extracted_path = './extracted_files/Validation/unzipped'\n",
    "\n",
    "# ZIP 파일 압축 해제\n",
    "extract_zip_files(train_zip_folder, train_extracted_path)\n",
    "extract_zip_files(val_zip_folder, val_extracted_path)\n",
    "\n",
    "# 캐시된 데이터를 로드 (없을 경우 병렬로 로드 후 캐시에 저장)\n",
    "train_qa_pairs = load_data_with_cache(train_extracted_path, 'train_data_cache.pkl')\n",
    "val_qa_pairs = load_data_with_cache(val_extracted_path, 'val_data_cache.pkl')\n",
    "\n",
    "# 데이터가 비어있는지 확인\n",
    "print(f\"Training 데이터 쌍 개수: {len(train_qa_pairs)}\")\n",
    "print(f\"Validation 데이터 쌍 개수: {len(val_qa_pairs)}\")\n",
    "\n",
    "if len(train_qa_pairs) == 0 or len(val_qa_pairs) == 0:\n",
    "    raise ValueError(\"데이터셋이 비어 있습니다. 데이터 로드 또는 전처리를 확인하세요.\")\n",
    "\n",
    "# 9. BERT 모델 및 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# 10. Dataset 및 DataLoader 설정\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, qa_pairs, tokenizer, max_len=512):\n",
    "        self.qa_pairs = qa_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qa_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question, context, answer, answer_start = self.qa_pairs[idx]\n",
    "\n",
    "        # 토크나이즈하고 필요한 인코딩 준비\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            question,\n",
    "            context,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # 정답에 대한 시작과 끝 위치 계산\n",
    "        answer_end = answer_start + len(self.tokenizer.encode(answer, add_special_tokens=False))\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"].squeeze()\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze()\n",
    "        token_type_ids = inputs[\"token_type_ids\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"start_positions\": torch.tensor(answer_start),\n",
    "            \"end_positions\": torch.tensor(answer_end)\n",
    "        }\n",
    "\n",
    "train_dataset = QADataset(train_qa_pairs, tokenizer)\n",
    "val_dataset = QADataset(val_qa_pairs, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# 11. 모델 학습 및 검증\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        start_positions = batch[\"start_positions\"].to(device)\n",
    "        end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            start_positions = batch[\"start_positions\"].to(device)\n",
    "            end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, start_positions=start_positions, end_positions=end_positions)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# 12. 학습 루프\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss = evaluate(model, val_loader, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# 13. Gradio 웹 인터페이스 구현\n",
    "def predict_answer(question, context):\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        start_scores = outputs.start_logits\n",
    "        end_scores = outputs.end_logits\n",
    "\n",
    "    start = torch.argmax(start_scores)\n",
    "    end = torch.argmax(end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][start:end]))\n",
    "    return answer\n",
    "\n",
    "def chatbot_interface(user_input):\n",
    "    context = \"국회 회의록에서의 발언 내용\"  # 실제 문맥으로 대체해야 함\n",
    "    answer = predict_answer(user_input, context)\n",
    "    return answer\n",
    "\n",
    "# 14. Gradio 웹 인터페이스 실행\n",
    "interface = gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"BERT 기반 국회 회의록 챗봇\")\n",
    "interface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
