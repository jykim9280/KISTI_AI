{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f3528b-f5e0-42e1-b6b6-73d0a0b9cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81e5e8b4-e515-4176-98db-293d5b561f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e52dc0d9-620a-4ae1-bf57-29777e13035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# 경고 메시지 무시 설정\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88877c8d-fab4-4939-8f96-96094a39abdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# 엑셀 파일들이 들어 있는 최상위 폴더 경로\n",
    "base_dir = r\"/mnt/c/study/KISTI_AI/023.국회 회의록 기반 지식검색 데이터/3.개방데이터/1.데이터/Training/01.원천데이터\"\n",
    "\n",
    "# 모든 하위 폴더 내 엑셀 파일 경로 검색\n",
    "excel_files = glob.glob(os.path.join(base_dir, '**', '*.xlsx'), recursive=True)\n",
    "\n",
    "qa_data = []  # JSON으로 저장할 데이터 리스트\n",
    "\n",
    "# 모든 엑셀 파일 읽기\n",
    "for file_path in excel_files:\n",
    "    try:\n",
    "        # 엑셀 파일 읽기\n",
    "        df = pd.read_excel(file_path)\n",
    "\n",
    "        question_info = None  # 현재 질문 정보를 저장할 변수\n",
    "\n",
    "        # 각 행을 순회하며 질문(Q)과 답변(A) 추출\n",
    "        for idx, row in df.iterrows():\n",
    "            if row['질의응답'] == 'Q':  # 질문일 때\n",
    "                question_info = {\n",
    "                    \"question\": row['발언내용'],\n",
    "                    \"회의번호\": row['회의번호'],\n",
    "                    \"질의응답번호\": row['질의응답번호'],\n",
    "                    \"회의구분\": row['회의구분'],\n",
    "                    \"위원회\": row['위원회'],\n",
    "                    \"회의일자\": row['회의일자'],\n",
    "                    \"질문자\": row['의원ID'],\n",
    "                    \"질문자_ISNI\": row['ISNI']\n",
    "                }\n",
    "            elif row['질의응답'] == 'A' and question_info is not None:  # 답변일 때\n",
    "                answer_info = {\n",
    "                    \"answer\": row['발언내용'],\n",
    "                    \"답변자\": row['의원ID'],\n",
    "                    \"답변자_ISNI\": row['ISNI']\n",
    "                }\n",
    "\n",
    "                # 유효성 검사: 질문과 답변이 있는지 확인\n",
    "                if not question_info['question'] or not answer_info['answer']:\n",
    "                    print(f\"파일 {file_path}의 {idx}번째 항목에서 질문 또는 답변이 누락되었습니다. 건너뜁니다.\")\n",
    "                    continue\n",
    "\n",
    "                # 질문과 답변을 연결하여 하나의 JSON 객체로 만듦\n",
    "                qa_data.append({**question_info, **answer_info})\n",
    "                question_info = None  # 사용 후 질문 정보를 초기화\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"파일을 찾을 수 없습니다: {file_path}\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"빈 파일이므로 건너뜁니다: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"파일 처리 중 오류 발생 ({file_path}): {e}\")\n",
    "\n",
    "# 라벨링 데이터 파일 저장 경로\n",
    "save_dir = r\"/mnt/c/study/KISTI_AI/023.국회 회의록 기반 지식검색 데이터/3.개방데이터/1.데이터/Training/02.라벨링데이터\"\n",
    "save_path = os.path.join(save_dir, '라벨링데이터.json')\n",
    "\n",
    "# 디렉토리가 존재하지 않으면 생성\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "try:\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(qa_data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"모든 파일 처리가 완료되었습니다. JSON 파일이 저장되었습니다: {save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"JSON 파일 저장 중 오류 발생: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654224a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# 라벨링된 JSON 파일 경로\n",
    "file_path = \"/mnt/c/study/KISTI_AI/023.국회 회의록 기반 지식검색 데이터/3.개방데이터/1.데이터/Training/02.라벨링데이터/라벨링데이터.json\"\n",
    "\n",
    "# JSON 파일 로드 및 예외 처리\n",
    "try:\n",
    "    # 파일 존재 여부 확인\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"파일을 찾을 수 없습니다. 경로를 확인해주세요: {file_path}\")\n",
    "\n",
    "    # JSON 파일 읽기\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        qa_data = json.load(f)\n",
    "    print(f\"JSON 파일이 정상적으로 로드되었습니다. 총 {len(qa_data)}개의 질문-답변 쌍이 있습니다.\")\n",
    "    \n",
    "    # 첫 5개의 질문-답변 쌍 확인\n",
    "    print(\"\\n첫 5개의 질문-답변 쌍 확인:\\n\")\n",
    "    for i, item in enumerate(qa_data[:5]):\n",
    "        print(f\"샘플 {i + 1}:\")\n",
    "        print(f\"질문: {item.get('question')}\")\n",
    "        print(f\"답변: {item.get('answer')}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"파일을 찾을 수 없습니다. 경로를 확인해주세요: {file_path}\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"JSON 파일을 읽는 중 오류가 발생했습니다. 파일 형식이 올바른지 확인해주세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec70e2-57c4-4c06-9977-244d9cdfddf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f0752-8bf9-4c71-8bd6-c4f3aa556d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3b3e46-2159-4af5-ade2-9c75fbcf6217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e52cb-c84e-4973-b27f-2149b377b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "다음 단계: 모델 학습 준비 및 학습\n",
    "데이터 전처리 및 준비:\n",
    "\n",
    "생성한 라벨링된 JSON 파일을 이용해 모델 학습에 적합한 형태로 변환해야 합니다.\n",
    "데이터를 질문-답변 쌍 형식으로 학습 데이터셋에 사용합니다.\n",
    "데이터를 나누어 **훈련 데이터(Training Set)**와 **검증 데이터(Validation Set)**로 나누는 작업을 합니다.\n",
    "모델 선택:\n",
    "\n",
    "질의응답 모델로 학습할 수 있는 사전 학습된 모델을 선택합니다. 대표적인 예시로는 다음과 같은 모델들이 있습니다:\n",
    "BERT (Bidirectional Encoder Representations from Transformers): 일반적으로 질의응답 태스크에 자주 사용됩니다.\n",
    "GPT (Generative Pre-trained Transformer): 대화형 모델과 자유 형식의 질문에 대해 답변을 생성하는 데 유용합니다.\n",
    "Hugging Face의 Transformers 라이브러리를 사용하면 다양한 사전 학습된 모델을 쉽게 사용할 수 있습니다.\n",
    "환경 설정 및 모델 학습:\n",
    "\n",
    "Python에서 모델을 학습할 수 있는 환경을 설정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489bf495-7f5b-4656-b343-06e07b925433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05b3329e-aac1-4b1f-86a0-97136c54c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bffb394e-4223-48d4-9e4b-90facf06a541",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch\n",
    "# !pip install torch\n",
    "# !pip install torch transformers konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3931efb8-c966-4c12-a738-41b864ebe0a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba27c347-e253-4c5e-9060-0ab2edba57a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "485e12a0-4510-4858-8211-6091f5a1248b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniforge3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1463, 30019,  ...,     0,     0,     0],\n",
      "        [  101,  1457, 30007,  ...,     0,     0,     0],\n",
      "        [  101,   100,  1464,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1463, 30010,  ..., 30006,   100,   102],\n",
      "        [  101,  1460, 30006,  ..., 29993, 30006,   102],\n",
      "        [  101,   100,  1457,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# Mecab 형태소 분석기 로드 (한국어 데이터의 경우)\n",
    "mecab = Mecab()\n",
    "\n",
    "# JSON 파일 경로 설정 (리눅스/WSL 경로로 수정)\n",
    "file_path = \"/mnt/c/study/KISTI_AI/023.국회 회의록 기반 지식검색 데이터/3.개방데이터/1.데이터/Training/02.라벨링데이터/라벨링데이터.json\"\n",
    "\n",
    "# JSON 파일 로드\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# BERT Tokenizer 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 불용어 정의\n",
    "stop_words = [\n",
    "    \"것\", \"있다\", \"하다\", \"입니다\", \"그리고\", \"하지만\", \"또한\", \"그런데\", \"저는\", \"우리는\", \"그래서\", \"이것\", \"저것\", \"그것\",\n",
    "    \"다시\", \"모든\", \"각각\", \"모두\", \"어느\", \"몇몇\", \"이런\", \"저런\", \"그런\", \"어떤\", \"특히\", \"즉\", \"또\", \"이후\", \"때문에\", \"통해서\",\n",
    "    \"같은\", \"많은\", \"따라서\", \"등\", \"경우\", \"관련\", \"대해\", \"의해\", \"이기\", \"대한\", \"그리고\", \"라고\", \"이라는\", \"에서\", \"부터\", \"까지\",\n",
    "    \"와\", \"과\", \"으로\", \"에\", \"의\", \"를\", \"가\", \"도\", \"로\", \"에게\", \"만\", \"뿐\", \"듯\", \"제\", \"내\", \"저\", \"그\", \"할\", \"수\", \"있\", \"같\",\n",
    "    \"되\", \"보다\", \"아니\", \"아닌\", \"이\", \"있어서\", \"입니다\", \"있습니다\", \"합니다\", \"입니까\", \"같습니다\", \"아닙니다\", \"라는\", \"그러므로\", \n",
    "    \"입니다만\", \"때문입니다\", \"라고요\", \"그러하다\", \"하고\", \"이와\"\n",
    "]\n",
    "\n",
    "# 데이터 정제 및 전처리 함수 정의\n",
    "def preprocess_text(text):\n",
    "    # 1. 특수 문자 및 불필요한 공백 제거\n",
    "    text = re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", text)  # 한글, 영문, 숫자, 공백만 남기기\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # 중복 공백 제거 및 양끝 공백 제거\n",
    "\n",
    "    # 2. 형태소 분석 및 불용어 제거\n",
    "    tokens = mecab.morphs(text)  # 형태소 분석\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]  # 불용어 제거\n",
    "\n",
    "    # 3. 정제된 단어들로 다시 결합\n",
    "    preprocessed_text = \" \".join(filtered_tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "# 데이터 전처리 및 학습 데이터 준비\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # 질문과 답변 전처리\n",
    "        question = preprocess_text(item['question'])\n",
    "        answer = preprocess_text(item['answer'])\n",
    "\n",
    "        # 질문과 답변을 토큰화\n",
    "        inputs = self.tokenizer(\n",
    "            question,\n",
    "            answer,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # 필요한 경우, 각 입력 데이터를 'squeeze()'로 차원 축소\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "# 데이터셋 준비\n",
    "dataset = QADataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# 데이터셋에서 샘플 확인\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cd980a-e598-443a-b568-21be4cc5a941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b38ed-a8b0-4413-b635-6f4ad06294b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6a2c3-cc7f-41a8-94e9-1c494a82ced7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ea686-698c-4f8a-b987-12e5f34b77f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d333254a-d803-4a43-874c-a697c3d4d38c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe84873-7051-49d9-a8a0-f00863d6eafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aea9c9-609c-46cf-8346-00408842aeee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b02693-46ab-4c6d-917c-52f01fc670b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
