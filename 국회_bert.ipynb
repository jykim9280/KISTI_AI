{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b91a8753-dfcc-4b0a-92db-5b6526e3f1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting gradio\n",
      "  Downloading gradio-5.0.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from gradio) (4.6.0)\n",
      "Collecting fastapi<1.0 (from gradio)\n",
      "  Downloading fastapi-0.115.2-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.4.0 (from gradio)\n",
      "  Downloading gradio_client-1.4.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from gradio) (0.27.2)\n",
      "Collecting markupsafe~=2.0 (from gradio)\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from gradio) (10.4.0)\n",
      "Collecting pydantic>=2.0 (from gradio)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio)\n",
      "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.31.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.0->gradio)\n",
      "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Collecting starlette<0.41.0,>=0.37.2 (from fastapi<1.0->gradio)\n",
      "  Downloading starlette-0.39.2-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: certifi in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0->gradio)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic>=2.0->gradio)\n",
      "  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting click>=8.0.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading rich-13.9.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/user/miniforge3/envs/nlp/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio-5.0.2-py3-none-any.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.4.0-py3-none-any.whl (319 kB)\n",
      "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading fastapi-0.115.2-py3-none-any.whl (94 kB)\n",
      "Downloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
      "Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Downloading uvicorn-0.31.1-py3-none-any.whl (63 kB)\n",
      "Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading networkx-3.4.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.2-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.39.2-py3-none-any.whl (73 kB)\n",
      "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pydub, mpmath, websockets, tqdm, tomlkit, sympy, shellingham, semantic-version, safetensors, ruff, regex, python-multipart, pydantic-core, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, mdurl, markupsafe, fsspec, filelock, ffmpy, click, annotated-types, aiofiles, uvicorn, triton, starlette, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, markdown-it-py, huggingface-hub, tokenizers, rich, nvidia-cusolver-cu12, gradio-client, fastapi, typer, transformers, torch, gradio\n",
      "  Attempting uninstall: markupsafe\n",
      "    Found existing installation: MarkupSafe 3.0.1\n",
      "    Uninstalling MarkupSafe-3.0.1:\n",
      "      Successfully uninstalled MarkupSafe-3.0.1\n",
      "Successfully installed aiofiles-23.2.1 annotated-types-0.7.0 click-8.1.7 fastapi-0.115.2 ffmpy-0.4.0 filelock-3.16.1 fsspec-2024.9.0 gradio-5.0.2 gradio-client-1.4.0 huggingface-hub-0.25.2 markdown-it-py-3.0.0 markupsafe-2.1.5 mdurl-0.1.2 mpmath-1.3.0 networkx-3.4.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.77 nvidia-nvtx-cu12-12.1.105 orjson-3.10.7 pydantic-2.9.2 pydantic-core-2.23.4 pydub-0.25.1 python-multipart-0.0.12 regex-2024.9.11 rich-13.9.2 ruff-0.6.9 safetensors-0.4.5 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.39.2 sympy-1.13.3 tokenizers-0.20.1 tomlkit-0.12.0 torch-2.4.1 tqdm-4.66.5 transformers-4.45.2 triton-3.0.0 typer-0.12.5 uvicorn-0.31.1 websockets-12.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers torch gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcf33959-1c70-4b0c-bcd1-713a8d4dc47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniforge3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gradio as gr\n",
    "import zipfile\n",
    "import pickle\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2493ee22-3d1f-40ad-952c-15d09b3bb682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일이 ./extracted_files 경로에 성공적으로 추출되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# .tar 파일 경로 지정\n",
    "tar_file_path = '/home/user/Downloads/download.tar'\n",
    "\n",
    "# .tar 파일을 추출할 디렉토리 지정\n",
    "extract_to = './extracted_files'\n",
    "\n",
    "# 추출할 디렉토리가 없으면 생성\n",
    "os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "# .tar 파일 열기 및 추출\n",
    "with tarfile.open(tar_file_path, 'r') as tar:\n",
    "    tar.extractall(path=extract_to)\n",
    "    print(f\"파일이 {extract_to} 경로에 성공적으로 추출되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bdf985-7d64-4d88-880d-741c8a29169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import zipfile\n",
    "\n",
    "# # Training과 Validation 경로 설정\n",
    "# train_label_data_path = './extracted_files/023.국회_회의록_기반_지식검색_데이터/3.개방데이터/1.데이터/Training/02.라벨링데이터'  # Training 하위의 라벨링 데이터 경로\n",
    "# val_label_data_path = './extracted_files/023.국회_회의록_기반_지식검색_데이터/3.개방데이터/1.데이터/Validation/02.라벨링데이터'  # Validation 하위의 라벨링 데이터 경로\n",
    "\n",
    "# # 압축을 풀 폴더 설정\n",
    "# train_extracted_path = './extracted_files/Training'  # Training 데이터 압축 해제 경로\n",
    "# val_extracted_path = './extracted_files/Validation'  # Validation 데이터 압축 해제 경로\n",
    "\n",
    "# # ZIP 파일 압축 해제 함수\n",
    "# def extract_zip_files(zip_folder, extract_to):\n",
    "#     zip_files = [f for f in os.listdir(zip_folder) if f.endswith('.zip')]  # 폴더 내 zip 파일만 필터링\n",
    "#     for zip_file in zip_files:\n",
    "#         zip_file_path = os.path.join(zip_folder, zip_file)\n",
    "#         with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "#             zip_ref.extractall(extract_to)\n",
    "#             print(f\"{zip_file} 압축이 성공적으로 해제되었습니다.\")\n",
    "\n",
    "# # Training 라벨링 데이터 압축 해제\n",
    "# extract_zip_files(train_label_data_path, train_extracted_path)\n",
    "\n",
    "# # Validation 라벨링 데이터 압축 해제\n",
    "# extract_zip_files(val_label_data_path, val_extracted_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2313ed61-41ec-40c1-8e93-ae8c7559115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TL_국정감사.zip.part0 병합 완료. ZIP 파일 저장 경로: ./extracted_files/Training/TL_국정감사.zip\n",
      "TL_본회의.zip.part0 병합 완료. ZIP 파일 저장 경로: ./extracted_files/Training/TL_본회의.zip\n",
      "TL_소위원회.zip.part0 병합 완료. ZIP 파일 저장 경로: ./extracted_files/Training/TL_소위원회.zip\n",
      "TL_예산결산특별위원회.zip.part0 병합 완료. ZIP 파일 저장 경로: ./extracted_files/Training/TL_예산결산특별위원회.zip\n",
      "TL_특별위원회.zip.part0 병합 완료. ZIP 파일 저장 경로: ./extracted_files/Training/TL_특별위원회.zip\n",
      "./extracted_files/Training/TL_국정감사.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Training/TL_국정감사\n",
      "./extracted_files/Training/TL_본회의.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Training/TL_본회의\n",
      "./extracted_files/Training/TL_소위원회.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Training/TL_소위원회\n",
      "./extracted_files/Training/TL_예산결산특별위원회.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Training/TL_예산결산특별위원회\n",
      "./extracted_files/Training/TL_특별위원회.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Training/TL_특별위원회\n",
      "VL_국정감사.zip.part0 병합 완료. ZIP 파일 저장 경로: ./extracted_files/Validation/VL_국정감사.zip\n",
      "VL_본회의.zip.part0 병합 완료. ZIP 파일 저장 경로: ./extracted_files/Validation/VL_본회의.zip\n",
      "VL_소위원회.zip.part0 병합 완료. ZIP 파일 저장 경로: ./extracted_files/Validation/VL_소위원회.zip\n",
      "VL_예산결산특별위원회.zip.part0 병합 완료. ZIP 파일 저장 경로: ./extracted_files/Validation/VL_예산결산특별위원회.zip\n",
      "VL_특별위원회.zip.part0 병합 완료. ZIP 파일 저장 경로: ./extracted_files/Validation/VL_특별위원회.zip\n",
      "./extracted_files/Validation/VL_국정감사.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Validation/VL_국정감사\n",
      "./extracted_files/Validation/VL_본회의.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Validation/VL_본회의\n",
      "./extracted_files/Validation/VL_소위원회.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Validation/VL_소위원회\n",
      "./extracted_files/Validation/VL_예산결산특별위원회.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Validation/VL_예산결산특별위원회\n",
      "./extracted_files/Validation/VL_특별위원회.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Validation/VL_특별위원회\n"
     ]
    }
   ],
   "source": [
    "# Training과 Validation 경로 설정\n",
    "train_part_folder = './extracted_files/023.국회_회의록_기반_지식검색_데이터/3.개방데이터/1.데이터/Training/02.라벨링데이터'\n",
    "val_part_folder = './extracted_files/023.국회_회의록_기반_지식검색_데이터/3.개방데이터/1.데이터/Validation/02.라벨링데이터'\n",
    "\n",
    "# 병합된 ZIP 파일이 저장될 경로 설정\n",
    "train_zip_output_folder = './extracted_files/Training'\n",
    "val_zip_output_folder = './extracted_files/Validation'\n",
    "\n",
    "# 병합된 ZIP 파일 저장 경로 설정\n",
    "train_output_zip_files = {\n",
    "    \"TL_국정감사.zip\": os.path.join(train_zip_output_folder, \"TL_국정감사.zip\"),\n",
    "    \"TL_본회의.zip\": os.path.join(train_zip_output_folder, \"TL_본회의.zip\"),\n",
    "    \"TL_소위원회.zip\": os.path.join(train_zip_output_folder, \"TL_소위원회.zip\"),\n",
    "    \"TL_예산결산특별위원회.zip\": os.path.join(train_zip_output_folder, \"TL_예산결산특별위원회.zip\"),\n",
    "    \"TL_특별위원회.zip\": os.path.join(train_zip_output_folder, \"TL_특별위원회.zip\"),\n",
    "}\n",
    "\n",
    "val_output_zip_files = {\n",
    "    \"VL_국정감사.zip\": os.path.join(val_zip_output_folder, \"VL_국정감사.zip\"),\n",
    "    \"VL_본회의.zip\": os.path.join(val_zip_output_folder, \"VL_본회의.zip\"),\n",
    "    \"VL_소위원회.zip\": os.path.join(val_zip_output_folder, \"VL_소위원회.zip\"),\n",
    "    \"VL_예산결산특별위원회.zip\": os.path.join(val_zip_output_folder, \"VL_예산결산특별위원회.zip\"),\n",
    "    \"VL_특별위원회.zip\": os.path.join(val_zip_output_folder, \"VL_특별위원회.zip\"),\n",
    "}\n",
    "\n",
    "# 1. 분할 파일을 병합하는 함수\n",
    "def merge_part_files(part_folder, output_file_prefix):\n",
    "    part_files = sorted([f for f in os.listdir(part_folder) if f.endswith('.part0')], key=lambda x: x)\n",
    "    for part_file in part_files:\n",
    "        zip_file_key = part_file.replace('.zip.part0', '.zip')  # 중복 확장자 방지\n",
    "        if zip_file_key in output_file_prefix:\n",
    "            zip_file = output_file_prefix[zip_file_key]\n",
    "\n",
    "            # 폴더가 존재하지 않으면 생성\n",
    "            os.makedirs(os.path.dirname(zip_file), exist_ok=True)\n",
    "            \n",
    "            part_file_path = os.path.join(part_folder, part_file)\n",
    "            with open(zip_file, 'wb') as merged_file:\n",
    "                with open(part_file_path, 'rb') as pf:\n",
    "                    merged_file.write(pf.read())\n",
    "                print(f\"{part_file} 병합 완료. ZIP 파일 저장 경로: {zip_file}\")\n",
    "        else:\n",
    "            print(f\"Warning: {zip_file_key}에 해당하는 출력 경로가 없습니다.\")\n",
    "\n",
    "# 2. ZIP 파일을 해제하는 함수\n",
    "def extract_zip_files(zip_file, extract_to):\n",
    "    if not os.path.exists(zip_file):\n",
    "        print(f\"ZIP 파일이 존재하지 않습니다: {zip_file}\")\n",
    "        return\n",
    "    \n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "        print(f\"{zip_file} 압축 해제 완료. 압축 해제 경로: {extract_to}\")\n",
    "\n",
    "# 3. Training 라벨링 데이터 병합 및 압축 해제\n",
    "merge_part_files(train_part_folder, train_output_zip_files)\n",
    "\n",
    "# Training ZIP 파일 해제\n",
    "for zip_file, output_path in train_output_zip_files.items():\n",
    "    extract_zip_files(output_path, os.path.join(train_zip_output_folder, os.path.splitext(zip_file)[0]))\n",
    "\n",
    "# 4. Validation 라벨링 데이터 병합 및 압축 해제\n",
    "merge_part_files(val_part_folder, val_output_zip_files)\n",
    "\n",
    "# Validation ZIP 파일 해제\n",
    "for zip_file, output_path in val_output_zip_files.items():\n",
    "    extract_zip_files(output_path, os.path.join(val_zip_output_folder, os.path.splitext(zip_file)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e9023db-1354-4d79-8146-b0daddcb7dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./extracted_files/Training/TL_국정감사.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Training/unzipped\n",
      "./extracted_files/Training/TL_본회의.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Training/unzipped\n",
      "./extracted_files/Training/TL_소위원회.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Training/unzipped\n",
      "./extracted_files/Training/TL_예산결산특별위원회.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Training/unzipped\n",
      "./extracted_files/Training/TL_특별위원회.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Training/unzipped\n",
      "./extracted_files/Validation/VL_국정감사.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Validation/unzipped\n",
      "./extracted_files/Validation/VL_본회의.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Validation/unzipped\n",
      "./extracted_files/Validation/VL_소위원회.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Validation/unzipped\n",
      "./extracted_files/Validation/VL_예산결산특별위원회.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Validation/unzipped\n",
      "./extracted_files/Validation/VL_특별위원회.zip 압축 해제 완료. 압축 해제 경로: ./extracted_files/Validation/unzipped\n",
      "데이터 폴더 경로: ./extracted_files/Training/unzipped\n",
      "발견된 파일들: 35233개\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 114\u001b[0m\n\u001b[1;32m    111\u001b[0m extract_zip_files(val_zip_folder, val_extracted_path)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# 캐시된 데이터를 로드 (없을 경우 병렬로 로드 후 캐시에 저장)\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m train_qa_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_extracted_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_data_cache.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m val_qa_pairs \u001b[38;5;241m=\u001b[39m load_data_with_cache(val_extracted_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_data_cache.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# 데이터가 비어있는지 확인\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 97\u001b[0m, in \u001b[0;36mload_data_with_cache\u001b[0;34m(data_folder, cache_file)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# 캐시된 데이터가 없으면 병렬로 데이터 로드\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m save_data_to_cache(data, cache_file)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "Cell \u001b[0;32mIn[9], line 80\u001b[0m, in \u001b[0;36mload_data_parallel\u001b[0;34m(data_folder)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m---> 80\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_single_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# None이 아닌 결과만 모으기\u001b[39;00m\n\u001b[1;32m     83\u001b[0m qa_pairs \u001b[38;5;241m=\u001b[39m [result \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "File \u001b[0;32m~/miniforge3/envs/nlp/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/miniforge3/envs/nlp/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/miniforge3/envs/nlp/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/miniforge3/envs/nlp/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Mecab 형태소 분석기 초기화\n",
    "mecab = MeCab.Tagger()\n",
    "\n",
    "# 2. ZIP 파일 압축 해제 함수\n",
    "def extract_zip_files(zip_folder, extract_to):\n",
    "    zip_files = [f for f in os.listdir(zip_folder) if f.endswith('.zip')]\n",
    "    for zip_file in zip_files:\n",
    "        zip_file_path = os.path.join(zip_folder, zip_file)\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "            print(f\"{zip_file_path} 압축 해제 완료. 압축 해제 경로: {extract_to}\")\n",
    "\n",
    "# 3. Mecab을 이용한 형태소 분석 및 전처리\n",
    "def preprocess_with_mecab(text):\n",
    "    \"\"\"Mecab을 사용하여 텍스트를 전처리하는 함수\"\"\"\n",
    "    words = mecab.parse(text).split()  # 형태소 단위로 나누기\n",
    "    return \" \".join(words)\n",
    "\n",
    "# 4. 데이터 로드 함수\n",
    "def load_single_file(file_path):\n",
    "    \"\"\"한 개의 JSON 파일을 로드하고 형태소 분석을 적용하는 함수\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            # 필요한 데이터를 처리\n",
    "            question = data.get('question', {}).get('comment')  # 질문 필드\n",
    "            context = data.get('context')  # 문맥 필드\n",
    "            answer = data.get('answer', {}).get('comment')  # 답변 필드\n",
    "\n",
    "            # Mecab으로 전처리\n",
    "            if question:\n",
    "                question = preprocess_with_mecab(question)\n",
    "            if context:\n",
    "                context = preprocess_with_mecab(context)\n",
    "            if answer:\n",
    "                answer = preprocess_with_mecab(answer)\n",
    "\n",
    "            # answer_start는 문맥 내에서 답변 시작 위치를 찾음\n",
    "            answer_start = context.find(answer) if answer in context else -1\n",
    "\n",
    "            if question and context and answer and answer_start != -1:\n",
    "                return (question, context, answer, answer_start)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON 디코딩 오류: {e}, 파일: {file_path}\")\n",
    "    return None\n",
    "\n",
    "# 5. 캐시 관련 함수\n",
    "CACHE_FILE = 'data_cache.pkl'\n",
    "\n",
    "def load_data_from_cache(cache_file):\n",
    "    \"\"\"캐시된 데이터를 불러오는 함수\"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"캐시된 데이터를 불러오는 중: {cache_file}\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def save_data_to_cache(data, cache_file):\n",
    "    \"\"\"데이터를 캐시에 저장하는 함수\"\"\"\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"데이터를 캐시에 저장했습니다: {cache_file}\")\n",
    "\n",
    "# 6. 병렬로 데이터 로드\n",
    "def load_data_parallel(data_folder):\n",
    "    \"\"\"여러 개의 파일을 병렬로 로드하는 함수\"\"\"\n",
    "    qa_pairs = []\n",
    "    print(f\"데이터 폴더 경로: {data_folder}\")\n",
    "\n",
    "    # 파일 리스트 가져오기\n",
    "    files = [os.path.join(root, file)\n",
    "             for root, _, files in os.walk(data_folder)\n",
    "             for file in files if file.endswith('.json')]\n",
    "\n",
    "    print(f\"발견된 파일들: {len(files)}개\")\n",
    "\n",
    "    # 각 파일을 병렬로 로드\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(load_single_file, files))\n",
    "\n",
    "    # None이 아닌 결과만 모으기\n",
    "    qa_pairs = [result for result in results if result is not None]\n",
    "\n",
    "    print(f\"로드된 QA 쌍 개수: {len(qa_pairs)}\")\n",
    "    return qa_pairs\n",
    "\n",
    "# 7. 캐시와 함께 데이터 로드 함수\n",
    "def load_data_with_cache(data_folder, cache_file):\n",
    "    \"\"\"캐시를 사용해 데이터를 로드하는 함수\"\"\"\n",
    "    # 캐시에서 데이터를 불러오려고 시도\n",
    "    data = load_data_from_cache(cache_file)\n",
    "    if data is not None:\n",
    "        return data\n",
    "\n",
    "    # 캐시된 데이터가 없으면 병렬로 데이터 로드\n",
    "    data = load_data_parallel(data_folder)\n",
    "    save_data_to_cache(data, cache_file)\n",
    "    return data\n",
    "\n",
    "# 8. 데이터 경로 설정 및 압축 해제\n",
    "train_zip_folder = './extracted_files/Training'\n",
    "val_zip_folder = './extracted_files/Validation'\n",
    "\n",
    "# 압축 해제 경로\n",
    "train_extracted_path = './extracted_files/Training/unzipped'\n",
    "val_extracted_path = './extracted_files/Validation/unzipped'\n",
    "\n",
    "# ZIP 파일 압축 해제\n",
    "extract_zip_files(train_zip_folder, train_extracted_path)\n",
    "extract_zip_files(val_zip_folder, val_extracted_path)\n",
    "\n",
    "# 캐시된 데이터를 로드 (없을 경우 병렬로 로드 후 캐시에 저장)\n",
    "train_qa_pairs = load_data_with_cache(train_extracted_path, 'train_data_cache.pkl')\n",
    "val_qa_pairs = load_data_with_cache(val_extracted_path, 'val_data_cache.pkl')\n",
    "\n",
    "# 데이터가 비어있는지 확인\n",
    "print(f\"Training 데이터 쌍 개수: {len(train_qa_pairs)}\")\n",
    "print(f\"Validation 데이터 쌍 개수: {len(val_qa_pairs)}\")\n",
    "\n",
    "if len(train_qa_pairs) == 0 or len(val_qa_pairs) == 0:\n",
    "    raise ValueError(\"데이터셋이 비어 있습니다. 데이터 로드 또는 전처리를 확인하세요.\")\n",
    "\n",
    "# 9. BERT 모델 및 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# 10. Dataset 및 DataLoader 설정\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, qa_pairs, tokenizer, max_len=512):\n",
    "        self.qa_pairs = qa_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qa_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question, context, answer, answer_start = self.qa_pairs[idx]\n",
    "\n",
    "        # 토크나이즈하고 필요한 인코딩 준비\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            question,\n",
    "            context,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # 정답에 대한 시작과 끝 위치 계산\n",
    "        answer_end = answer_start + len(self.tokenizer.encode(answer, add_special_tokens=False))\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"].squeeze()\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze()\n",
    "        token_type_ids = inputs[\"token_type_ids\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"start_positions\": torch.tensor(answer_start),\n",
    "            \"end_positions\": torch.tensor(answer_end)\n",
    "        }\n",
    "\n",
    "train_dataset = QADataset(train_qa_pairs, tokenizer)\n",
    "val_dataset = QADataset(val_qa_pairs, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# 11. 모델 학습 및 검증\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        start_positions = batch[\"start_positions\"].to(device)\n",
    "        end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            start_positions = batch[\"start_positions\"].to(device)\n",
    "            end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, start_positions=start_positions, end_positions=end_positions)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# 12. 학습 루프\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss = evaluate(model, val_loader, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# 13. Gradio 웹 인터페이스 구현\n",
    "def predict_answer(question, context):\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        start_scores = outputs.start_logits\n",
    "        end_scores = outputs.end_logits\n",
    "\n",
    "    start = torch.argmax(start_scores)\n",
    "    end = torch.argmax(end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][start:end]))\n",
    "    return answer\n",
    "\n",
    "def chatbot_interface(user_input):\n",
    "    context = \"국회 회의록에서의 발언 내용\"  # 실제 문맥으로 대체해야 함\n",
    "    answer = predict_answer(user_input, context)\n",
    "    return answer\n",
    "\n",
    "# 14. Gradio 웹 인터페이스 실행\n",
    "interface = gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"BERT 기반 국회 회의록 챗봇\")\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378afc12-f28c-47b3-9604-89f730aca1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 폴더 경로: ./extracted_files/Training/unzipped\n",
      "발견된 파일들: 35233개\n",
      "로드된 QA 쌍 개수: 8594\n",
      "데이터를 캐시에 저장했습니다: train_data_cache.pkl\n",
      "데이터 폴더 경로: ./extracted_files/Validation/unzipped\n",
      "발견된 파일들: 4400개\n",
      "로드된 QA 쌍 개수: 1081\n",
      "데이터를 캐시에 저장했습니다: val_data_cache.pkl\n",
      "Training 데이터 쌍 개수: 8594\n",
      "Validation 데이터 쌍 개수: 1081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/miniforge3/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1523 > 512). Running this sequence through the model will result in indexing errors\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gradio as gr\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import MeCab\n",
    "\n",
    "# 1. Mecab 형태소 분석기 초기화\n",
    "mecab = MeCab.Tagger()\n",
    "\n",
    "# 2. Mecab을 이용한 형태소 분석 및 전처리\n",
    "def preprocess_with_mecab(text):\n",
    "    \"\"\"Mecab을 사용하여 텍스트를 전처리하는 함수\"\"\"\n",
    "    words = mecab.parse(text).split()\n",
    "    return \" \".join(words)\n",
    "\n",
    "# 3. 데이터 로드 함수\n",
    "def load_single_file(file_path):\n",
    "    \"\"\"한 개의 JSON 파일을 로드하고 형태소 분석을 적용하는 함수\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            # 필요한 데이터를 처리\n",
    "            question = data.get('question', {}).get('comment')  # 질문 필드\n",
    "            context = data.get('context')  # 문맥 필드\n",
    "            answer = data.get('answer', {}).get('comment')  # 답변 필드\n",
    "\n",
    "            # Mecab으로 전처리\n",
    "            if question:\n",
    "                question = preprocess_with_mecab(question)\n",
    "            if context:\n",
    "                context = preprocess_with_mecab(context)\n",
    "            if answer:\n",
    "                answer = preprocess_with_mecab(answer)\n",
    "\n",
    "            # answer_start는 문맥 내에서 답변 시작 위치를 찾음\n",
    "            answer_start = context.find(answer) if answer in context else -1\n",
    "\n",
    "            if question and context and answer and answer_start != -1:\n",
    "                return (question, context, answer, answer_start)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON 디코딩 오류: {e}, 파일: {file_path}\")\n",
    "    return None\n",
    "\n",
    "# 4. 캐시 관련 함수\n",
    "CACHE_FILE = 'data_cache.pkl'\n",
    "\n",
    "def load_data_from_cache(cache_file):\n",
    "    \"\"\"캐시된 데이터를 불러오는 함수\"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"캐시된 데이터를 불러오는 중: {cache_file}\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def save_data_to_cache(data, cache_file):\n",
    "    \"\"\"데이터를 캐시에 저장하는 함수\"\"\"\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"데이터를 캐시에 저장했습니다: {cache_file}\")\n",
    "\n",
    "# 5. 병렬로 데이터 로드\n",
    "def load_data_parallel(data_folder):\n",
    "    \"\"\"여러 개의 파일을 병렬로 로드하는 함수\"\"\"\n",
    "    qa_pairs = []\n",
    "    print(f\"데이터 폴더 경로: {data_folder}\")\n",
    "\n",
    "    # 파일 리스트 가져오기\n",
    "    files = [os.path.join(root, file)\n",
    "             for root, _, files in os.walk(data_folder)\n",
    "             for file in files if file.endswith('.json')]\n",
    "\n",
    "    print(f\"발견된 파일들: {len(files)}개\")\n",
    "\n",
    "    # 각 파일을 병렬로 로드\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(load_single_file, files))\n",
    "\n",
    "    # None이 아닌 결과만 모으기\n",
    "    qa_pairs = [result for result in results if result is not None]\n",
    "\n",
    "    print(f\"로드된 QA 쌍 개수: {len(qa_pairs)}\")\n",
    "    return qa_pairs\n",
    "\n",
    "# 6. 캐시와 함께 데이터 로드 함수\n",
    "def load_data_with_cache(data_folder, cache_file):\n",
    "    \"\"\"캐시를 사용해 데이터를 로드하는 함수\"\"\"\n",
    "    # 캐시에서 데이터를 불러오려고 시도\n",
    "    data = load_data_from_cache(cache_file)\n",
    "    if data is not None:\n",
    "        return data\n",
    "\n",
    "    # 캐시된 데이터가 없으면 병렬로 데이터 로드\n",
    "    data = load_data_parallel(data_folder)\n",
    "    save_data_to_cache(data, cache_file)\n",
    "    return data\n",
    "\n",
    "# 7. 데이터 경로 설정\n",
    "train_extracted_path = './extracted_files/Training/unzipped'\n",
    "val_extracted_path = './extracted_files/Validation/unzipped'\n",
    "\n",
    "# 캐시된 데이터를 로드 (없을 경우 병렬로 로드 후 캐시에 저장)\n",
    "train_qa_pairs = load_data_with_cache(train_extracted_path, 'train_data_cache.pkl')\n",
    "val_qa_pairs = load_data_with_cache(val_extracted_path, 'val_data_cache.pkl')\n",
    "\n",
    "# 데이터가 비어있는지 확인\n",
    "print(f\"Training 데이터 쌍 개수: {len(train_qa_pairs)}\")\n",
    "print(f\"Validation 데이터 쌍 개수: {len(val_qa_pairs)}\")\n",
    "\n",
    "if len(train_qa_pairs) == 0 or len(val_qa_pairs) == 0:\n",
    "    raise ValueError(\"데이터셋이 비어 있습니다. 데이터 로드 또는 전처리를 확인하세요.\")\n",
    "\n",
    "# 8. BERT 모델 및 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# 9. Dataset 및 DataLoader 설정\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, qa_pairs, tokenizer, max_len=512):\n",
    "        self.qa_pairs = qa_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qa_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question, context, answer, answer_start = self.qa_pairs[idx]\n",
    "\n",
    "        # 토크나이즈하고 필요한 인코딩 준비\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            question,\n",
    "            context,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # 정답에 대한 시작과 끝 위치 계산\n",
    "        answer_end = answer_start + len(self.tokenizer.encode(answer, add_special_tokens=False))\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"].squeeze()\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze()\n",
    "        token_type_ids = inputs[\"token_type_ids\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"start_positions\": torch.tensor(answer_start),\n",
    "            \"end_positions\": torch.tensor(answer_end)\n",
    "        }\n",
    "\n",
    "train_dataset = QADataset(train_qa_pairs, tokenizer)\n",
    "val_dataset = QADataset(val_qa_pairs, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# 10. 모델 학습 및 검증\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        start_positions = batch[\"start_positions\"].to(device)\n",
    "        end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            start_positions = batch[\"start_positions\"].to(device)\n",
    "            end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, start_positions=start_positions, end_positions=end_positions)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# 11. 학습 루프\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss = evaluate(model, val_loader, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# 12. Gradio 웹 인터페이스 구현\n",
    "def predict_answer(question, context):\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        start_scores = outputs.start_logits\n",
    "        end_scores = outputs.end_logits\n",
    "\n",
    "    start = torch.argmax(start_scores)\n",
    "    end = torch.argmax(end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][start:end]))\n",
    "    return answer\n",
    "\n",
    "def chatbot_interface(user_input):\n",
    "    context = \"국회 회의록에서의 발언 내용\"  # 실제 문맥으로 대체해야 함\n",
    "    answer = predict_answer(user_input, context)\n",
    "    return answer\n",
    "\n",
    "# 13. Gradio 웹 인터페이스 실행\n",
    "interface = gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"BERT 기반 국회 회의록 챗봇\")\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e7f2d-0957-4b92-a1c8-803b20c1d62d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94916bc7-91cf-401d-9164-702aaf8a22f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b40540-12c1-4c12-ae14-5b7fbb155915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc21fcfc-cd20-4b9a-bf17-a9fef001408f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
